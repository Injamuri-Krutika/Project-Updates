<div class="container research">
  <h1 class="jumbotron-heading text-center">
    Progress of our Research
  </h1>
  <div class="row research">
    <div class="card col-12 card-shadow">
      <div class="card-header">Date: 19th August 2019</div>
      <div class="card-body">
        <p class="card-text">
            <ul>
                <li>Tasks completed so far:</li>
                <ul>
                    <li>Recreated the <a href="https://arxiv.org/pdf/1907.02591.pdf">Algonauts challenge MEG winner's paper</a></li>
                    <li>Trained AlexNet with and without foveation</li>
                    <li>Used this trained model to check the results of Algonauts challenge</li>
                    <li>Observed huge improvement in the results by applying foveation</li>
                </ul>
                <li>Below are the things to be worked on:</li>
                <ul>
                    <li>Create a proper logger</li>
                    <li>Debugg the exploading weights in case of VGG</li>
                    <li>Work on the MEG implementation</li>
                    <li>Train all the networks</li>
                    <li>Make foveation as one of the layers in the network instead of pre-processing step</li>
                </ul>
            </ul>
        </p>
      </div>
    </div>
  </div>
  <div class="row research">
    <div class="card col-12 card-shadow">
      <div class="card-header">Date: 7th September 2019</div>
      <div class="card-body">
          <p class="card-text">
              <h3>Experimenting with the foveation operation</h3>
              <p>Git Repository: <a href="https://github.com/dnn-hvs/Siamese-Network" target="blank">Siamese Network</a>, 
                <a href="https://github.com/dnn-hvs/Foveated-Transform" target="blank">Foveation</a></p>
              <p>Results: <a href="https://drive.google.com/open?id=1iky2Z-kl2sBcDb67pwHpN9rzPRIePbYu" target="blank">Drive link</a></p>

              <h5> What was done?</h5>
              <p>The main focus for this part of the experiment was to find a neural network that best corelates with the RDMs we have.
                The experiments were performed over many variants of the network. The major parameters that changed were as follows:
              </p>
                <ul>
                    <li>The task: fMRI or MEG</li>
                    <li>Wheather Foveation was applied or not</li>
                    <li>Wheather the layers of the network were frozen during fine-tuning</li>
                    <li>Wheather the network was trained for early or later regions</li>
                </ul>
              <p>Now this gives us 16 variatons for each type of model. After the fine tuning process, we create features from which RDMs can be formed.</p>
              <ul>
                  <b><li>Training setup:</li></b>
                    <p>We considered two famous architectures, AlexNet and VGG16 each of them is setup as 16 combinations as described above.
                      Each one of them was then trained for 150 epochs.
                    </p>
                    While training we saved both the best performing and the latest models. The best performing model was the one that had the least loss.
                    <p>
                    </p>
                  <b><li>Results obtained:</li></b>
                  <ul>
                    <li><b><u>The following parameters are used for evaluation:</u></b></li>
                    <ol>
                      <li><b>fMRI</b>
                        <ul>
                          <li>EVC R <sup>2</sup></li> 
                          <li>EVC Significance</li>
                          <li>EVC % Noise Ceiling</li>
                          <li>IT R <sup>2</sup></li>
                          <li>IT Significance</li>
                          <li>IT % Noise Ceiling</li>                         
                          <li>fMRI Average R <sup>2</sup></li>
                          <li>fMRI Average % Noise Ceiling</li>
                        </ul>
                      </li>
                      <li><b>MEG</b>
                        <ul>
                          <li>Early R <sup>2</sup></li> 
                          <li>Early Significance</li>
                          <li>Early % Noise Ceiling</li>
                          <li>Late R <sup>2</sup></li>
                          <li>Late Significance</li>
                          <li>Late % Noise Ceiling</li>                         
                          <li>MEG Average R <sup>2</sup></li>
                          <li>MEG Average % Noise Ceiling</li>
                        </ul>
                      </li>
                    </ol>
                    <li><b><u>Observations:</u></b></li>
                    <p>Below we give results for both the 92 and the 118 image sets</p>
                    <ul>
                      <li><b><i>92 Image Set:</i></b></li>
                      <ol>
                          <li><b>fMRI</b>
                            <ul>
                              <li>EVC R<sup>2</sup>: vgg16_fmri_early_nofov_unfrozen_bestfmri92 with 0.06145370247</li> 
                              <li>EVC Significance: alexnet_meg_late_nofov_unfrozen_bestfmri92 with 0.04258330791</li>
                              <li>EVC % Noise Ceiling: vgg16_fmri_early_nofov_unfrozen_bestfmri92 with 38.67445089</li>
                              <li>IT R<sup>2</sup>: alexnet_fmri_late_nofov_unfrozen_bestfmri92 with 0.1183017672</li>
                              <li>IT Significance: alexnet_fmri_late_nofov_frozen_bestfmri92 with 0.02285354092</li>
                              <li>IT % Noise Ceiling: alexnet_fmri_late_nofov_unfrozen_bestfmri92 with 38.47211941</li>                         
                              <li>fMRI Average R<sup>2</sup>: alexnet_fmri_late_nofov_unfrozen_bestfmri92 with 0.06661316812</li>
                              <li>fMRI Average % Noise Ceiling: alexnet_fmri_late_nofov_unfrozen_bestfmri92 with 28.56482338</li>
                            </ul>
                          </li>
                          <li><b>MEG</b>
                            <ul>
                              <li>Early R<sup>2</sup>: vgg16_fmri_early_nofov_unfrozen_bestmeg92 with 0.1455016278</li> 
                              <li>Early Significance: alexnet_fmri_late_nofov_frozen_bestmeg92 with 0.07310470618</li>
                              <li>Early % Noise Ceiling: vgg16_fmri_early_nofov_unfrozen_bestmeg92 with 31.39871123</li>
                              <li>Late R<sup>2</sup>: alexnet_fmri_late_fov_unfrozen_bestmeg92 with 0.06131034014</li>
                              <li>Late Significance: alexnet_meg_late_fov_frozen_bestmeg92 with 0.01607485701</li>
                              <li>Late % Noise Ceiling: alexnet_fmri_late_fov_unfrozen_bestmeg92 with 26.94960006</li>                         
                              <li>MEG Average R<sup>2</sup>: alexnet_fmri_early_nofov_frozen_bestmeg92 with 0.09258170773</li>
                              <li>MEG Average % Noise Ceiling: alexnet_fmri_early_nofov_frozen_bestmeg92 with 26.80032066</li>
                            </ul>
                          </li>
                        </ol>
                      <li><b><i>118 Image Set:</i></b></li>
                      <ol>
                          <li><b>fMRI</b>
                            <ul>
                              <li>EVC R<sup>2</sup>: vgg16_fmri_early_nofov_unfrozen_bestfmri118 with 0.04052828943</li> 
                              <li>EVC Significance: vgg16_fmri_early_nofov_unfrozen_bestfmri118 with 0.06236890943</li>
                              <li>EVC % Noise Ceiling: vgg16_fmri_early_nofov_unfrozen_bestfmri118 with 38.6720319</li>
                              <li>IT R<sup>2</sup>: alexnet_fmri_late_fov_unfrozen_bestfmri118 with 0.01145429889</li>
                              <li>IT Significance: alexnet_fmri_early_nofov_unfrozen_bestfmri118 with 0.06783343543</li>
                              <li>IT % Noise Ceiling: alexnet_fmri_late_nofov_unfrozen_bestfmri118 with 12.75121378</li>                         
                              <li>fMRI Average R<sup>2</sup>: vgg16_fmri_early_nofov_unfrozen_bestfmri118 with 0.02184186614</li>
                              <li>fMRI Average % Noise Ceiling: vgg16_fmri_early_nofov_unfrozen_bestfmri118 with 24.5966961</li>
                            </ul>
                          </li>
                          <li><b>MEG</b>
                            <ul>
                              <li>Early R <sup>2</sup></li> 
                              <li>Early Significance</li>
                              <li>Early % Noise Ceiling: vgg16_fmri_early_nofov_unfrozen_bestmeg118 with 37.36458679</li>
                              <li>Late R<sup>2</sup>: alexnet_fmri_early_nofov_unfrozen_bestmeg118 with 0.03017670713</li>
                              <li>Late Significance: alexnet_fmri_early_nofov_unfrozen_bestmeg118 with 0.1154116251</li>
                              <li>Late % Noise Ceiling: alexnet_fmri_early_nofov_unfrozen_bestmeg118 with 13.3230495</li>                         
                              <li>MEG Average R<sup>2</sup>: vgg16_fmri_early_nofov_unfrozen_bestmeg118 with 0.07918802147</li>
                              <li>MEG Average % Noise Ceiling: vgg16_fmri_early_nofov_unfrozen_bestmeg118 with 27.62533454</li>
                            </ul>
                          </li>
                        </ol>
                    </ul>
                  </ul>
              </ul>
          </p>
      </div>
    </div>
  </div>
  <div class="row research">
      <div class="card col-12 card-shadow">
        <div class="card-header">Date: 14th September 2019</div>
        <div class="card-body">
            <p class="card-text">
                <h3>Experimenting on RDMs</h3>
                <p>Git Repository: <a href="https://github.com/dnn-hvs/Investigate-RDMs" target="blank">Investigate-RDMs</a></p>
                <p>Results: <a href="https://drive.google.com/open?id=1-f01VlNDFrwoa-BftHZ6OeMY3N7eZAGw" target="blank">PDFs of the Results</a></p>

                <h5> What was done?</h5>
                <p>The task was divided into three parts</p>
                  <ul>
                      <li>Investigation of fMRI RDMs</li>
                      <li>Investigation of MEG RDMs with mean across the Subjects (15)</li>
                      <li>Investigation of MEG RDMs with maximum and minimum of RDMs across the Subjects (15)</li>
                  </ul>
                <p>Now lets see, what was done in each one of them and what were the results.</p>
                <ul>
                    <b><li>fMRI Investigation:</li></b>
                    <ul>
                      <li><u>Experiment conducted:</u></li>
                      <p>For each of the 15 subjects, we have considered the 10 most similar image pairs (10 least RMD values)
                        and 10 most dissimilar pairs (10 highest RDM values)
                      </p>
                      <li><u>Observations:</u></li>
                      <p>There are no major observations in fMRI RDMs investigation. The image pairs are different for different subjects.
                        Very few sceneraies, mokey faces, human faces are paired.
                      </p>
                    </ul>
                    <b><li>MEG Investigation with mean:</li></b>
                    <ul>
                      <li><u>Experiment conducted:</u></li>
                      <p>The mean of RDMs was taken across the 15 subjects.
                        Then, we have considered the 10 most similar image pairs (10 least RMD values)
                        and 10 most dissimilar pairs (10 highest RDM values)
                      </p>
                      <li><u>Observations:</u></li>
                      <ul>
                        <li><i>MEG Early for 92 images:</i></li>
                        <p>All the subjects show almost similar results with a very littele variations in the RDM values.
                          Few of the similiar images have same shape.
                        </p>
                        <li><i>MEG Late for 92 images:</i></li>
                        <p>Face pairs have lower RDM values. The image pairs for higher RDMS are also the same for all subjects.</p>
                        <li><i>MEG Early for 118 images:</i></li>
                        <p>Almost all the subjects have same image pairs. They have same shape.</p>
                        <li><i>MEG Late for 118 images:</i></li>
                        <p>Machines, Means of transport, animals are paired together.The background of few image pairs are also similar.</p>
                      </ul>
                    </ul>
                    <b><li>MEG Investigation with max and min:</li></b>
                    <ul>
                      <li><u>Experiment conducted:</u></li>
                      <p>For the similar image pairs, an RDM(say "Min RDM") is created with lowest values across the 15 subjects (resulting in matrix sizes of 20x92x92, 20x118x118). 
                        And for dissimilar image pairs, another RDM(say "Max RDM") is created with highest values across the 15 subjects.
                        Then, we have considered the 10 most similar image pairs (10 least RMD values from "Min RDM")
                        and 10 most dissimilar pairs (10 highest RDM values from "Max RDM")
                      </p>
                      <li><u>Observations:</u></li>
                      Similar to the observations in MEG with mean, all the time points have almost same pairs.
                      <ul>
                        <li><i>MEG Early 92 images:</i></li>
                        Images with similar faces, shapes , scenes were paired.
                        <li><i>MEG Late 92 images:</i></li>
                        Human and dog faces were paired.
                        <li><i>MEG Early 118 images:</i></li>
                        Few of the similar image pairs have same shape. Few pairs are same as those in the MEG mean eperiment.
                        <li><i>MEG Late 118 images:</i></li>
                        It does not make much sense. Same image pairs observed at almost all the time points.
                      </ul>
                    </ul>
                </ul>
            </p>
        </div>
      </div>
  </div>
  <div class="row research">
    <div class="card col-12 card-shadow">
      <div class="card-header">Date: 17th September 2019</div>
      <div class="card-body">
          <p class="card-text">
              <h3>Experimenting on RDMs Contd...</h3>
              <p>Git Repository: <a href="https://github.com/dnn-hvs/Investigate-RDMs" target="blank">Investigate-RDMs</a></p>
              <p>Results: <a href="https://drive.google.com/drive/folders/1SLsUrcb_YeAqB7sN4T6FmF51BVMxtRzd?usp=sharing" target="blank">PDFs of the Results</a></p>

              <h5> What was done?</h5>
              <p>The previous task was continued further to investigate on the MEG rdms with mean and mix-max operations across the time points for every subject</p>
                <ul>
                    <li>Investigation of MEG RDMs with mean across the time points (20)</li>
                    <li>Investigation of MEG RDMs with maximum and minimum of RDMs across the time points (20)</li>
                </ul>
              <p>Now lets see, what was done in each one of them and what were the results.</p>
              <ul>
                  <b><li>MEG Investigation with mean:</li></b>
                  <ul>
                    <li><u>Experiment conducted:</u></li>
                    <p>The mean of RDMs was taken across the 20 time points for every subject.
                      Then, we have considered the 5 most similar image pairs (5 least RMD values)
                      and 5 most dissimilar pairs (5 highest RDM values). 
                      For these image pairs, the corresponding change in the RDMs w.r.t the time points is plotted
                      for all the subject and for the mean of all subjects.
                    </p>
                    <li><u>Observations:</u></li>
                    <ul>
                      <li><i>MEG Early for 92 images:</i> We see patterns such as shape, size, faces</li>
                      <li><i>MEG Late for 92 images:</i> Human faces and animlas are paired together</li>
                      <li><i>MEG Early for 118 images:</i> No conculsive similaities</li>
                      <li><i>MEG Late for 118 images:</i> Apart from animals, the other similaities are something that only the subjects understand</li>
                    </ul>
                  </ul>
                  <b><li>MEG Investigation with max and min:</li></b>
                  <ul>
                    <li><u>Experiment conducted:</u></li>
                    <p>For the similar image pairs, an RDM(say "Min RDM") is created with lowest values across the 20 time points (resulting in matrix sizes of 15x92x92, 15x118x118). 
                      And for dissimilar image pairs, another RDM(say "Max RDM") is created with highest values across the 20 time points.
                      Then, we have considered the 5 most similar image pairs (5 least RMD values from "Min RDM")
                      and 5 most dissimilar pairs (5 highest RDM values from "Max RDM"). We have also plotted the graphs same as in the above experiment.
                    </p>
                    <li><u>Observations:</u></li>
                    Similar to the observations in MEG with mean, all the time points have almost same pairs.
                    <ul>
                      <li><i>MEG Early 92 images:</i> We cordially invite SHerlock holmes to solve this mystery</li>
                      <li><i>MEG Late 92 images:</i> Human faces are paired.The subjects love camels(Paired mostly as similar) and hate snakes(Paired mostly as dissimilar)</li>
                      <li><i>MEG Early 118 images:</i> The subjects are beyond redemption</li>
                      Does not make much sense.
                      <li><i>MEG Late 118 images:</i> Americans love for all th pets, burgur, and pizza is clearly highlighted</li>
                      Animals, Pizza and Burger, Machines, Means of transport are paired.
                    </ul>
                  </ul>
              </ul>
          </p>
      </div>
    </div>
  </div>
  <div class="row research">
    <div class="card col-12 card-shadow">
      <div class="card-header">Date: 6th October 2019</div>
      <div class="card-body">
          <p class="card-text">
              <h3>Salient Maps for Similar and Dissimilar image pairs</h3>
              <p>Git Repository: <a href="https://github.com/dnn-hvs/Visualise" target="blank">Visualise</a></p>
              <p>Results: <a href="#" target="blank">PDFs of the Results</a></p>

              <h5> What was done?</h5>
              <p>
                <ol>
                  <li>Salient Maps were created for pretrained models and fine-tuned models</li>
                  <li>Salient maps for Similar and Dissimilar image pairs for few best performing models</li>
                </ol>
              </p>
              <u><h5>Salient Maps for Pretrained models</h5></u>
              <p>
                Below are the observations from the Salient Maps for different models.
                <ul>
                  <li>The Salient maps were created for Squeezenet, Alexnet and VGG</li>
                  <li>More noise was observed for a lady bug image for Squeenet model.</li>
                  <li>The Salient maps for Alextnet were meaningful focusing on the black spots of the lady bug</li>
                  <li>And those of VGG were focusing on a very small area, which made no sense at all.</li>
                </ul>
              </p>
              <u><h5>Salient maps for fine-tuned models</h5></u>
              <p>
                <ul>
                  <li>Few Salient maps for Squeezenet were blank.</li>
                  <li>For Alexnet, they were similar to that of the pretrained model.</li>
                  <li>And for VGG the focus maps didnt make any sense.</li>
                </ul>
              </p>
              <u><h5>Salient maps for Similar and Dissimilar image pairs</h5></u>
              <p>
                In this, the Similar and Dissimilar image pairs of the fine-tuned models that 
                performed well in the Algonauts challenge test were taken into consideration.
                Salient maps were created for these image pairs. <br>
                Models that were experimented:
                <ul>
                  <li>alexnet_fmri_early_fov_frozen_best</li>
                  <li>alexnet_fmri_early_fov_unfrozen_best</li>
                  <li>alexnet_fmri_early_nofov_unfrozen_best</li>
                  <li>alexnet_fmri_late_fov_unfrozen_best</li>
                  <li>alexnet_fmri_late_nofov_frozen_best</li>
                  <li>alexnet_fmri_late_nofov_unfrozen_best</li>
                </ul>
                Following observations were made for <b>alexnet_fmri_early_nofov_unfrozen_best</b>:
                <ul>
                  <li>
                      In the later layers of alexnet the similar images are not quite related.
                  </li>
                  <li>In the initial layers of alexnet the similar images have same shape.</li>
                  <li>
                      In the initial layers of Alexnet the RDM values of similar image pairs is as high as 0.4210, 0.4495, 0.4897, 0.3844 and 0.2601 for 1, 3, 7, 11 and 13th layers respectively.
                  </li>
                </ul>
                <p>Similar observations were made for all the other models.The image pairs of all the models in the initial layers were similar. They were mainly shape based.
                  Nothing significant can be found from the area of focus for the similar image pairs in the later layers. 
                  But the image pairs are related to means of transport like scooter and train, and also musicl instruments.
                </p>
              </p>
          </p>
      </div>
    </div>
  </div>
</div>
